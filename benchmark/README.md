# CCO Benchmark Suite

Comprehensive benchmark system for measuring CCO's real-world impact on software projects.

## Purpose

This benchmark answers the critical question: **Does CCO provide measurable, concrete value?**

It compares code generated by:
- **Vanilla Claude Code**: `ccbox --bare` (no rules, pure Claude)
- **CCO Claude Code**: `ccbox:base` (with CCO rules and context)

## Projects

| # | Project | Stack | Categories | Complexity |
|---|---------|-------|------------|------------|
| 1 | REST API Service | Python/FastAPI | Backend:FastAPI, API:REST, DB:ORM | High |
| 2 | CLI Dashboard | Python/Textual | T:CLI, DEP:TUI, DEP:Config | Medium |
| 3 | React Dashboard | TypeScript/React | Frontend:React, L:TypeScript | High |
| 4 | Utility Library | TypeScript | T:Library, Build:TypeChecker | Medium |
| 5 | WebSocket Chat | Go | API:WebSocket, RT:Basic | Medium |
| 6 | GraphQL API | Python/Strawberry | API:GraphQL, DB:SQL, DEP:Auth | High |
| 7 | Monorepo Starter | TypeScript | Build:Monorepo, CI:GitHub | Medium |
| 8 | ML Pipeline | Python | ML:Inference, DEP:DataQuery | High |

## Quick Start

### Web UI (Recommended)

```bash
cd benchmark
pip install -r requirements.txt
python -m benchmark

# Open http://localhost:8765
```

### CLI Mode

```bash
# List projects
python -m benchmark --list

# Run full benchmark for a project
python -m benchmark --run 01_rest_api

# Run only vanilla or cco
python -m benchmark --run 01_rest_api --variant vanilla

# View summary
python -m benchmark --summary
```

## Metrics Collected

### Quantitative (Automatic)
| Metric | Description |
|--------|-------------|
| LOC | Total lines of code |
| Test Coverage | Percentage of code covered by tests |
| Type Coverage | Percentage of typed parameters/returns |
| Complexity | Cyclomatic complexity (avg/max) |
| Function Count | Total functions and methods |

### Quality Indicators (Automatic)
| Indicator | Good = | Bad = |
|-----------|--------|-------|
| Bare Excepts | 0 | Any |
| Silent Passes | 0 | Any |
| Giant Functions (>50 LOC) | 0 | Any |
| Exception Chains | High | 0 |
| Context Managers | High | 0 |
| Type Annotations | High | 0 |

### UX/DX Scores (Calculated)
| Score | Measures |
|-------|----------|
| Modularity | Function size distribution (smaller = better) |
| SRP | Single responsibility (focused functions) |
| Error Handling | Quality of exception handling |
| Naming | Quality of variable/function names |

## Results Interpretation

### Score Calculation

```
Score = 50 (base)
      - bare_excepts * 5
      - silent_passes * 10
      - giant_funcs * 8
      + exception_chains * 2
      + context_managers * 0.5
      + type_coverage * 0.15
      + test_coverage * 0.1
      + modularity * 0.05
      + srp * 0.05
```

### Verdicts

| Difference | Verdict |
|------------|---------|
| +15 or more | Strong CCO Advantage |
| +5 to +14 | Moderate CCO Advantage |
| -4 to +4 | Mixed Results |
| -14 to -5 | Moderate Vanilla Advantage |
| -15 or less | Strong Vanilla Advantage |

## Directory Structure

```
/
├── app/                    # Web UI
│   ├── server.py          # FastAPI backend
│   └── templates/         # HTML dashboard
├── projects/              # 8 benchmark projects
│   ├── 01_rest_api/
│   │   ├── PROMPT.md     # Task prompt
│   │   └── SPEC.md       # Expected metrics
│   └── ...
├── runner/                # Execution engine
│   ├── executor.py       # ccbox integration
│   └── metrics.py        # Code analysis
├── results/              # JSON results
├── output/               # Generated projects
└── requirements.txt
```

## Test Execution Flow

```
1. Load PROMPT.md for project
2. Run ccbox --bare (vanilla) → output/project_vanilla_timestamp/
3. Analyze generated code → metrics
4. Run ccbox:base (cco) → output/project_cco_timestamp/
5. Analyze generated code → metrics
6. Compare metrics → verdict
7. Save results/project_timestamp.json
```

## Web UI Features

- **Projects Tab**: Select projects, view prompts (copyable)
- **Results Tab**: Compare scores, times, detailed metrics
- **Summary Tab**: Aggregate statistics, win rates

### API Endpoints

```
GET  /api/projects              # List projects
GET  /api/projects/{id}         # Project details
GET  /api/projects/{id}/prompt  # Get prompt text
POST /api/run                   # Start benchmark
GET  /api/status/{run_id}       # Check progress
GET  /api/results               # List results
GET  /api/results/{filename}    # Get full result
GET  /api/summary               # Aggregate stats
```

## Important Notes

1. **Run outside CCO directory**: Benchmarks should run in isolated directories to avoid CCO rules affecting vanilla tests

2. **Same prompt, different context**: Both variants receive identical prompts; only the system context differs

3. **Timing includes all**: Generation time includes model response + any tool use

4. **Determinism**: Results may vary between runs due to model non-determinism

## Example Result

```json
{
  "project_id": "01_rest_api",
  "project_name": "01 Rest Api",
  "verdict": "Moderate CCO Advantage",
  "score_difference": 12.3,
  "cco_result": {
    "score": 78.5,
    "generation_time_seconds": 245.3,
    "metrics": {
      "total_loc": 1247,
      "functions": 45,
      "giant_funcs": 0,
      "type_coverage_pct": 92.3,
      "bare_excepts": 0,
      "exception_chains": 8
    }
  },
  "vanilla_result": {
    "score": 66.2,
    "generation_time_seconds": 198.7,
    "metrics": {
      "total_loc": 1089,
      "functions": 38,
      "giant_funcs": 3,
      "type_coverage_pct": 67.5,
      "bare_excepts": 2,
      "exception_chains": 1
    }
  }
}
```
