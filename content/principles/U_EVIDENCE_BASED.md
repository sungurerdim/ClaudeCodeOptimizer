---
id: U_EVIDENCE_BASED
title: Evidence-Based Verification
category: universal
severity: critical
weight: 10
enforcement: SHOULD
applicability:
  project_types: ['all']
  languages: ['all']
---

# U_EVIDENCE_BASED: Evidence-Based Verification üî¥

**Severity**: Critical

Never claim completion without command execution proof. All verification requires fresh command output with exit codes.

**Enforcement**: SHOULD

**Project Types**: all
**Languages**: all

---

## Why

### The Problem
- **Assumption-based development** leads to 60%+ of production bugs (Gartner)
- "It should work" without verification causes cascading failures
- Memory/assumption errors compound during AI-assisted development
- Unverified claims create technical debt that's expensive to fix
- Silent failures mask critical issues until production

### Verification Requirements

**Every claim MUST be supported by:**
1. **Command execution output** (not assumptions)
2. **Exit codes** (0 = success, non-zero = failure)
3. **Timestamps** (prove freshness of verification)
4. **Error messages** (when applicable)

### Implementation Patterns

#### ‚úÖ Good: Execute and Verify
```bash
# Run test and capture output
$ pytest tests/test_auth.py -v
================================ test session starts =================================
collected 12 items

tests/test_auth.py::test_login_valid PASSED                                    [  8%]
tests/test_auth.py::test_login_invalid FAILED                                  [ 16%]
================================ 1 failed, 11 passed =================================
$ echo $?
1

# Evidence: Test ran, 1 failure detected, exit code 1 confirms failure
```

#### ‚úÖ Good: Build Verification
```bash
# Build and verify success
$ npm run build
‚úì 243 modules compiled successfully
Build completed in 4.2s
$ echo $?
0

# Evidence: Build succeeded, exit code 0 confirms success
```

#### ‚úÖ Good: Integration Test
```bash
# Verify API integration
$ curl -s -o /dev/null -w "%{http_code}" https://api.example.com/health
200
$ echo $?
0

# Evidence: API returned 200, curl succeeded (exit 0)
```

---

## Anti-Patterns

### ‚ùå Bad: Assumption-Based Claims
```plaintext
"I fixed the authentication bug."
- NO proof of execution
- NO test output
- NO verification
- UNACCEPTABLE
```

### ‚ùå Bad: Visual Inspection Only
```plaintext
"I looked at the code and it looks correct."
- NO execution
- NO exit codes
- Assumptions, not evidence
- UNACCEPTABLE
```

### ‚ùå Bad: Cached/Old Output
```plaintext
"The tests passed yesterday, so it should be fine."
- NOT fresh verification
- Doesn't prove CURRENT state
- UNACCEPTABLE
```

### ‚ùå Bad: Partial Verification
```plaintext
"I ran the test but didn't check the exit code."
- Incomplete evidence
- Exit codes are REQUIRED
- UNACCEPTABLE
```

---

## Implementation Checklist

- [ ] **Before claiming completion:** Run the command
- [ ] **Capture full output:** Not just summary
- [ ] **Check exit code:** `echo $?` (bash), `$LASTEXITCODE` (PowerShell)
- [ ] **Verify timestamps:** Ensure fresh execution
- [ ] **Document failures:** Include error messages
- [ ] **Re-run after fixes:** Prove the fix works

---

## Examples by Scenario

### Scenario 1: Test Verification
```bash
# ‚ùå Bad
"Tests pass"

# ‚úÖ Good
$ pytest tests/ -v --tb=short
================================ test session starts =================================
collected 156 items

tests/test_auth.py ‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì [12/156]
tests/test_api.py ‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì [27/156]
...
================================ 156 passed in 12.3s =================================
$ echo $?
0
```

### Scenario 2: Build Verification
```bash
# ‚ùå Bad
"Build successful"

# ‚úÖ Good
$ cargo build --release
   Compiling myproject v0.1.0
    Finished release [optimized] target(s) in 8.45s
$ echo $?
0
```

### Scenario 3: Linter Verification
```bash
# ‚ùå Bad
"No linting errors"

# ‚úÖ Good
$ ruff check . --fix
All checks passed!
$ echo $?
0
```

### Scenario 4: Deployment Verification
```bash
# ‚ùå Bad
"Deployed successfully"

# ‚úÖ Good
$ kubectl rollout status deployment/myapp
deployment "myapp" successfully rolled out
$ echo $?
0
```

---

## Metrics and Monitoring

### Key Indicators
- **Verification coverage:** % of claims with evidence
- **Failure detection rate:** Issues caught before production
- **False positive rate:** Claims proven wrong by evidence
- **Time to feedback:** Execution to verification time

### Success Criteria
- 100% of completion claims have command output
- 100% of verifications include exit codes
- Zero assumptions about "should work" without proof
- All failures detected before code review

---

## Tools and Automation

### Built-in Verification
```bash
# Bash: Always check exit codes
command_to_verify
if [ $? -eq 0 ]; then
    echo "‚úì Verified: Success"
else
    echo "‚úó Failed: Exit code $?"
    exit 1
fi
```

### Python: subprocess with verification
```python
import subprocess

result = subprocess.run(['pytest', 'tests/'], capture_output=True)
if result.returncode == 0:
    print(f"‚úì Tests passed\n{result.stdout.decode()}")
else:
    print(f"‚úó Tests failed (exit {result.returncode})\n{result.stderr.decode()}")
    raise SystemExit(1)
```

### CI/CD: Automated Evidence Collection
```yaml
# GitHub Actions
- name: Run Tests with Evidence
  run: |
    pytest tests/ -v --junitxml=test-results.xml
    echo "Exit code: $?"
- name: Upload Test Evidence
  uses: actions/upload-artifact@v3
  with:
    name: test-evidence
    path: test-results.xml
```

---

## Common Pitfalls

1. **"Trust me" syndrome** - Skip verification because "I know it works"
2. **Time pressure shortcuts** - Skip evidence to save time (causes more time later)
3. **Overconfidence bias** - Assume correctness without verification
4. **Verification fatigue** - Skip verification on "simple" changes (they break most often)
5. **Tool over-reliance** - Trust IDE indicators without actual execution

---

## Migration Path

### Phase 1: Awareness (Week 1)
- Identify all assumption-based claims in current workflow
- Document verification gaps

### Phase 2: Manual Verification (Week 2-4)
- Execute commands for all completion claims
- Capture and document output

### Phase 3: Automation (Week 5+)
- Add verification to CI/CD pipelines
- Create verification scripts/tools
- Enforce via PR checks

---

## Summary

**Evidence-Based Verification** eliminates assumptions by requiring proof through command execution and exit codes. Every completion claim MUST be backed by fresh, verifiable output.
